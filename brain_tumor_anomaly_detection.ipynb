{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "brain_tumor_anomaly_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b312bea609d940b8bac330eeb5e87289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aa809a24e29f4d5c878624aa51b9ce86",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4bb381671ebc44129f78fc3d08aa3265",
              "IPY_MODEL_cfb3da9e83294645baf141807fc1eb01",
              "IPY_MODEL_d24c458722e3429bb238d0f8bbe35bcb"
            ]
          }
        },
        "aa809a24e29f4d5c878624aa51b9ce86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "4bb381671ebc44129f78fc3d08aa3265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0229e1b3fa9463a9acc0c7de149298c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 2: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31b6ff1b1a7b47dfbc65d4d4c2eea0f1"
          }
        },
        "cfb3da9e83294645baf141807fc1eb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_945960b992854559b9ad2916c64814d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 125,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 125,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f81c60e4aa344879bf20ac7c130267d3"
          }
        },
        "d24c458722e3429bb238d0f8bbe35bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_94fe01d368ef4f47a31d2207113cad5a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 125/125 [06:02&lt;00:00,  2.88s/it, loss=nan, v_num=0]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0d480a52e27417c9138654e1d71d9dc"
          }
        },
        "d0229e1b3fa9463a9acc0c7de149298c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31b6ff1b1a7b47dfbc65d4d4c2eea0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "945960b992854559b9ad2916c64814d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f81c60e4aa344879bf20ac7c130267d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94fe01d368ef4f47a31d2207113cad5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0d480a52e27417c9138654e1d71d9dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "846192b2a0c24cd9864886ba306c7811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2fec08c99f349d292c8fb3738106950",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0dbb64ae59df4fa4be7c4d41636c56f1",
              "IPY_MODEL_cb6546774f5044e6b061879bae86604a",
              "IPY_MODEL_a2310b4fe73b4f188f1ab7a887dec7a6"
            ]
          }
        },
        "e2fec08c99f349d292c8fb3738106950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "0dbb64ae59df4fa4be7c4d41636c56f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c6f8fdc0303467c83d9ba9e13619727",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Testing: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_863d58abbb4045bab5b586e9d9e668a5"
          }
        },
        "cb6546774f5044e6b061879bae86604a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8bba117d2354d85882a09602c94feda",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9ccb795959d45ca87cc3b51aae5eb40"
          }
        },
        "a2310b4fe73b4f188f1ab7a887dec7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3daca2246794a58b3b579d188f19081",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca09c023f42346b5ae6033d8ad3b5028"
          }
        },
        "4c6f8fdc0303467c83d9ba9e13619727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "863d58abbb4045bab5b586e9d9e668a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8bba117d2354d85882a09602c94feda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9ccb795959d45ca87cc3b51aae5eb40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3daca2246794a58b3b579d188f19081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca09c023f42346b5ae6033d8ad3b5028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHCxWo3SfB9"
      },
      "source": [
        "!unzip dataset/dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzBOOJT3pRaL"
      },
      "source": [
        "!sudo rm -rf \"image log\" sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cwprWSQmPjB",
        "outputId": "88f29cf1-c807-4ccc-f4ef-640d6118b886"
      },
      "source": [
        "!pip install --target=$my_path pytorch_lightning\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pickle\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from torchvision.models import wide_resnet50_2\n",
        "from scipy.ndimage import gaussian_filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2021.9.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.6.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.5.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.40.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_xnZT-TTW-"
      },
      "source": [
        "\"\"\"Abstract class for sampling methods.\n",
        "Provides interface to sampling methods that allow same signature\n",
        "for select_batch.  Each subclass implements select_batch_ with the desired\n",
        "signature for readability.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class SamplingMethod(object):\n",
        "  __metaclass__ = abc.ABCMeta\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __init__(self, X, y, seed, **kwargs):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.seed = seed\n",
        "\n",
        "  def flatten_X(self):\n",
        "    shape = self.X.shape\n",
        "    flat_X = self.X\n",
        "    if len(shape) > 2:\n",
        "      flat_X = np.reshape(self.X, (shape[0],np.product(shape[1:])))\n",
        "    return flat_X\n",
        "\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def select_batch_(self):\n",
        "    return\n",
        "\n",
        "  def select_batch(self, **kwargs):\n",
        "    return self.select_batch_(**kwargs)\n",
        "\n",
        "  def to_dict(self):\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1lsKGmqTMHp"
      },
      "source": [
        "\"\"\"Returns points that minimizes the maximum distance of any point to a center.\n",
        "Implements the k-Center-Greedy method in\n",
        "Ozan Sener and Silvio Savarese.  A Geometric Approach to Active Learning for\n",
        "Convolutional Neural Networks. https://arxiv.org/abs/1708.00489 2017\n",
        "Distance metric defaults to l2 distance.  Features used to calculate distance\n",
        "are either raw features or if a model has transform method then uses the output\n",
        "of model.transform(X).\n",
        "Can be extended to a robust k centers algorithm that ignores a certain number of\n",
        "outlier datapoints.  Resulting centers are solution to multiple integer program.\n",
        "\"\"\"\n",
        "class kCenterGreedy(SamplingMethod):\n",
        "\n",
        "  def __init__(self, X, y, seed, metric='euclidean'):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.flat_X = self.flatten_X()\n",
        "    self.name = 'kcenter'\n",
        "    self.features = self.flat_X\n",
        "    self.metric = metric\n",
        "    self.min_distances = None\n",
        "    self.n_obs = self.X.shape[0]\n",
        "    self.already_selected = []\n",
        "\n",
        "  def update_distances(self, cluster_centers, only_new=True, reset_dist=False):\n",
        "    \"\"\"Update min distances given cluster centers.\n",
        "    Args:\n",
        "      cluster_centers: indices of cluster centers\n",
        "      only_new: only calculate distance for newly selected points and update\n",
        "        min_distances.\n",
        "      rest_dist: whether to reset min_distances.\n",
        "    \"\"\"\n",
        "\n",
        "    if reset_dist:\n",
        "      self.min_distances = None\n",
        "    if only_new:\n",
        "      cluster_centers = [d for d in cluster_centers\n",
        "                         if d not in self.already_selected]\n",
        "    if cluster_centers:\n",
        "      # Update min_distances for all examples given new cluster center.\n",
        "      x = self.features[cluster_centers]\n",
        "      dist = pairwise_distances(self.features, x, metric=self.metric)\n",
        "\n",
        "      if self.min_distances is None:\n",
        "        self.min_distances = np.min(dist, axis=1).reshape(-1,1)\n",
        "      else:\n",
        "        self.min_distances = np.minimum(self.min_distances, dist)\n",
        "\n",
        "  def select_batch_(self, model, already_selected, N, **kwargs):\n",
        "    \"\"\"\n",
        "    Diversity promoting active learning method that greedily forms a batch\n",
        "    to minimize the maximum distance to a cluster center among all unlabeled\n",
        "    datapoints.\n",
        "    Args:\n",
        "      model: model with scikit-like API with decision_function implemented\n",
        "      already_selected: index of datapoints already selected\n",
        "      N: batch size\n",
        "    Returns:\n",
        "      indices of points selected to minimize distance to cluster centers\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "      # Assumes that the transform function takes in original data and not\n",
        "      # flattened data.\n",
        "      print('Getting transformed features...')\n",
        "      self.features = model.transform(self.X)\n",
        "      print('Calculating distances...')\n",
        "      self.update_distances(already_selected, only_new=False, reset_dist=True)\n",
        "    except:\n",
        "      print('Using flat_X as features.')\n",
        "      self.update_distances(already_selected, only_new=True, reset_dist=False)\n",
        "\n",
        "    new_batch = []\n",
        "\n",
        "    for _ in range(N):\n",
        "      if self.already_selected is None:\n",
        "        # Initialize centers with a randomly selected datapoint\n",
        "        ind = np.random.choice(np.arange(self.n_obs))\n",
        "      else:\n",
        "        ind = np.argmax(self.min_distances)\n",
        "      # New examples should not be in already selected since those points\n",
        "      # should have min_distance of zero to a cluster center.\n",
        "      assert ind not in already_selected\n",
        "\n",
        "      self.update_distances([ind], only_new=True, reset_dist=False)\n",
        "      new_batch.append(ind)\n",
        "    print('Maximum distance from cluster centers is %0.2f'\n",
        "            % max(self.min_distances))\n",
        "\n",
        "\n",
        "    self.already_selected = already_selected\n",
        "\n",
        "    return new_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSu9uTafTgZv"
      },
      "source": [
        "# Utilty functions\n",
        "\n",
        "def distance_matrix(x, y=None, p=2):  # pairwise distance of vectors\n",
        "\n",
        "\ty = x if type(y) == type(None) else y\n",
        "\tn = x.size(0)\n",
        "\tm = y.size(0)\n",
        "\td = x.size(1)\n",
        " \n",
        "\tx = x.unsqueeze(1).expand(n, m, d)\n",
        "\ty = y.unsqueeze(0).expand(n, m, d)\n",
        "\n",
        "\tdist = torch.pow(x - y, p).sum(2)\n",
        "\n",
        "\treturn dist\n",
        "\n",
        "def copy_files(src, dst, ignores=[]):\n",
        "\tsrc_files = os.listdir(src)\n",
        "\tfor file_name in src_files:\n",
        "\t\tignore_check = [True for i in ignores if i in file_name]\n",
        "\t\tif ignore_check:\n",
        "\t\t\tcontinue\n",
        "\t\tfull_file_name = os.path.join(src, file_name)\n",
        "\t\tif os.path.isfile(full_file_name):\n",
        "\t\t\tshutil.copy(full_file_name, os.path.join(dst,file_name))\n",
        "\t\tif os.path.isdir(full_file_name):\n",
        "\t\t\tos.makedirs(os.path.join(dst, file_name), exist_ok=True)\n",
        "\t\t\tcopy_files(full_file_name, os.path.join(dst, file_name), ignores)\n",
        "\n",
        "def prep_dirs(root):\n",
        "\t# make embeddings dir\n",
        "\t# embeddings_path = os.path.join(root, 'embeddings')\n",
        "\tembeddings_path = os.path.join('./', args['model_path'])\n",
        "\tos.makedirs(embeddings_path, exist_ok=True)\n",
        "\t# make sample dir\n",
        "\tsample_path = os.path.join(root, 'sample')\n",
        "\tos.makedirs(sample_path, exist_ok=True)\n",
        "\t# make source code record dir & copy\n",
        "\tsource_code_save_path = os.path.join(root, 'src')\n",
        "\tos.makedirs(source_code_save_path, exist_ok=True)\n",
        "\t# copy_files('./', source_code_save_path, ['.git','.vscode','__pycache__','logs','README','samples','LICENSE']) # copy source code\n",
        "\treturn embeddings_path, sample_path, source_code_save_path\n",
        "\n",
        "def cvt2heatmap(gray):\n",
        "\theatmap = cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET)\n",
        "\treturn heatmap\n",
        "\n",
        "def heatmap_on_image(heatmap, image):\n",
        "\tif heatmap.shape != image.shape:\n",
        "\t\theatmap = cv2.resize(heatmap, (image.shape[0], image.shape[1]))\n",
        "\tout = np.float32(heatmap)/255 + np.float32(image)/255\n",
        "\tout = out / np.max(out)\n",
        "\treturn np.uint8(255 * out)\n",
        "\n",
        "def min_max_norm(image):\n",
        "    a_min, a_max = image.min(), image.max()\n",
        "    # a_min, a_max = 0.32, 3.4\n",
        "    return a_min, a_max, (image-a_min)/(a_max - a_min)\n",
        "\n",
        "def min_max_norm2(image, min_value, max_value):\n",
        "    image_min, image_max = image.min(), image.max()\n",
        "    return image_min, image_max, (image-min_value)/(max_value - min_value)   \n",
        "\n",
        "\n",
        "def cal_confusion_matrix(y_true, y_pred_no_thresh, thresh, img_path_list):\n",
        "\tpred_thresh = []\n",
        "\tfalse_n = []\n",
        "\tfalse_p = []\n",
        "\tfor i in range(len(y_pred_no_thresh)):\n",
        "\t\tif y_pred_no_thresh[i] > thresh:\n",
        "\t\t\tpred_thresh.append(1)\n",
        "\t\t\tif y_true[i] == 0:\n",
        "\t\t\t\tfalse_p.append(img_path_list[i])\n",
        "\t\telse:\n",
        "\t\t\tpred_thresh.append(0)\n",
        "\t\t\tif y_true[i] == 1:\n",
        "\t\t\t\tfalse_n.append(img_path_list[i])\n",
        "\n",
        "\tcm = confusion_matrix(y_true, pred_thresh)\n",
        "\tprint(cm)\n",
        "\tprint('false positive')\n",
        "\tprint(false_p)\n",
        "\tprint('false negative')\n",
        "\tprint(false_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00BVqXYTrqA"
      },
      "source": [
        "# Neural Network Classes\n",
        "class NN():\n",
        "\n",
        "\tdef __init__(self, X=None, Y=None, p=2):\n",
        "\t\tself.p = p\n",
        "\t\tself.train(X, Y)\n",
        "\n",
        "\tdef train(self, X, Y):\n",
        "\t\tself.train_pts = X\n",
        "\t\tself.train_label = Y\n",
        "\n",
        "\tdef __call__(self, x):\n",
        "\t\treturn self.predict(x)\n",
        "\n",
        "\tdef predict(self, x):\n",
        "\t\tif type(self.train_pts) == type(None) or type(self.train_label) == type(None):\n",
        "\t\t\tname = self.__class__.__name__\n",
        "\t\t\traise RuntimeError(f\"{name} wasn't trained. Need to execute {name}.train() first\")\n",
        "\n",
        "\t\tdist = distance_matrix(x, self.train_pts, self.p) ** (1 / self.p)\n",
        "\t\tlabels = torch.argmin(dist, dim=1)\n",
        "\t\treturn self.train_label[labels]\n",
        "\n",
        "class KNN(NN):\n",
        "\n",
        "\tdef __init__(self, X=None, Y=None, k=3, p=2):\n",
        "\t\tself.k = k\n",
        "\t\tsuper().__init__(X, Y, p)\n",
        "\n",
        "\tdef train(self, X, Y):\n",
        "\t\tsuper().train(X, Y)\n",
        "\t\tif type(Y) != type(None):\n",
        "\t\t\tself.unique_labels = self.train_label.unique()\n",
        "\n",
        "\tdef predict(self, x):\n",
        "\t\tdist = distance_matrix(x, self.train_pts, self.p) ** (1 / self.p)\n",
        "\t\tknn = dist.topk(self.k, largest=False)\n",
        "\t\treturn knn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQgZaeeKTvkP"
      },
      "source": [
        "def embedding_concat(x, y):\n",
        "\t# from https://github.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master\n",
        "\tB, C1, H1, W1 = x.size()\n",
        "\t_, C2, H2, W2 = y.size()\n",
        "\ts = int(H1 / H2)\n",
        "\tx = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
        "\tx = x.view(B, C1, -1, H2, W2)\n",
        "\tz = torch.zeros(B, C1 + C2, x.size(2), H2, W2)\n",
        "\tfor i in range(x.size(2)):\n",
        "\t\tz[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
        "\tz = z.view(B, -1, H2 * W2)\n",
        "\tz = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
        "\n",
        "\treturn z\n",
        "\n",
        "def reshape_embedding(embedding):\n",
        "\tembedding_list = []\n",
        "\tfor k in range(embedding.shape[0]):\n",
        "\t\tfor i in range(embedding.shape[2]):\n",
        "\t\t\tfor j in range(embedding.shape[3]):\n",
        "\t\t\t\tembedding_list.append(embedding[k, :, i, j])\n",
        "\treturn embedding_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cm78AVITxz9"
      },
      "source": [
        "#imagenet\n",
        "mean_train = [0.485, 0.456, 0.406]\n",
        "std_train = [0.229, 0.224, 0.225]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVQWKnCQUfA0"
      },
      "source": [
        "# DataLoader Class\n",
        "class MVTecDataset(Dataset):\n",
        "\tdef __init__(self, root, transform, phase):\n",
        "\t\tif phase=='train':\n",
        "\t\t\tself.img_path = os.path.join(root, 'train')\n",
        "\t\telse:\n",
        "\t\t\tself.img_path = os.path.join(root, 'test')\n",
        "\t\tself.transform = transform\n",
        "\t\t# load dataset\n",
        "\t\tself.img_paths, self.labels, self.types = self.load_dataset() # self.labels => good : 0, anomaly : 1\n",
        "\n",
        "\tdef load_dataset(self):\n",
        "\n",
        "\t\timg_tot_paths = []\n",
        "\t\ttot_labels = []\n",
        "\t\ttot_types = []\n",
        "\n",
        "\t\tdefect_types = os.listdir(self.img_path)\n",
        "\t\t\n",
        "\t\tfor defect_type in defect_types:\n",
        "\t\t\tif defect_type == 'good':\n",
        "\t\t\t\timg_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.*\")\n",
        "\t\t\t\timg_tot_paths.extend(img_paths)\n",
        "\t\t\t\ttot_labels.extend([0]*len(img_paths))\n",
        "\t\t\t\ttot_types.extend(['good']*len(img_paths))\n",
        "\t\t\telse:\n",
        "\t\t\t\timg_paths = glob.glob(os.path.join(self.img_path, defect_type) + \"/*.*\")\n",
        "\t\t\t\timg_paths.sort()\n",
        "\t\t\t\timg_tot_paths.extend(img_paths)\n",
        "\t\t\t\ttot_labels.extend([1]*len(img_paths))\n",
        "\t\t\t\ttot_types.extend([defect_type]*len(img_paths))\n",
        "\n",
        "\t\t\n",
        "\t\treturn img_tot_paths, tot_labels, tot_types\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.img_paths)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\timg_path, label, img_type = self.img_paths[idx], self.labels[idx], self.types[idx]\n",
        "\t\timg = Image.open(img_path).convert('RGB')\n",
        "\t\timg = self.transform(img)\n",
        "\n",
        "\t\treturn img, label, os.path.basename(img_path[:-4]), img_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grk6HYMUUjJZ"
      },
      "source": [
        "# model architecture\n",
        "class STPM(pl.LightningModule):\n",
        "\tdef __init__(self, hparams):\n",
        "\t\tsuper(STPM, self).__init__()\n",
        "\n",
        "\t\tself.save_hyperparameters(hparams)\n",
        "\n",
        "\t\tself.init_features()\n",
        "\t\tdef hook_t(module, input, output):\n",
        "\t\t\tself.features.append(output)\n",
        "\n",
        "\t\t# self.model = torch.hub.load('pytorch/vision:v0.9.0', 'wide_resnet50_2', pretrained=True)\n",
        "\t\tself.model = wide_resnet50_2(pretrained = True)\n",
        "\n",
        "\t\tfor param in self.model.parameters():\n",
        "\t\t\tparam.requires_grad = False\n",
        "\n",
        "\t\tself.model.layer2[-1].register_forward_hook(hook_t)\n",
        "\t\tself.model.layer3[-1].register_forward_hook(hook_t)\n",
        "\n",
        "\t\tself.criterion = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "\t\tself.init_results_list()\n",
        "\n",
        "\t\tself.data_transforms = transforms.Compose([\n",
        "\t\t\t\t\t\ttransforms.Resize((args['load_size'], args['load_size']), Image.ANTIALIAS),\n",
        "\t\t\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\t\t\ttransforms.CenterCrop(args['input_size'])])\n",
        "\t\t\t\t\t\t# transforms.Normalize(mean=mean_train,\n",
        "\t\t\t\t\t\t# \t\t\t\t\tstd=std_train)]) \n",
        "\n",
        "\t\t# self.inv_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255])\n",
        "\n",
        "\tdef init_results_list(self):\n",
        "\t\tself.pred_list_px_lvl = []\n",
        "\t\tself.pred_list_img_lvl = []\n",
        "\t\tself.img_path_list = []        \n",
        "\n",
        "\tdef init_features(self):\n",
        "\t\tself.features = []\n",
        "\n",
        "\tdef forward(self, x_t):\n",
        "\t\tself.init_features()\n",
        "\t\t_ = self.model(x_t)\n",
        "\t\treturn self.features\n",
        "\n",
        "\tdef save_anomaly_map(self, anomaly_map, input_img, file_name, x_type):\n",
        "\t\tif anomaly_map.shape != input_img.shape:\n",
        "\t\t\tanomaly_map = cv2.resize(anomaly_map, (input_img.shape[0], input_img.shape[1]))\n",
        "\t\tanomaly_map_norm = min_max_norm(anomaly_map)\n",
        "\t\tanomaly_map_norm_hm = cvt2heatmap(anomaly_map_norm*255)\n",
        "\t\t\n",
        "\t\t# print(anomaly_map_norm_hm)\n",
        "\t\t(H, W) = input_img.shape[:2]\n",
        "\n",
        "\t\t# save images\n",
        "\t\tcv2.imwrite(f'{self.sample_path}/{x_type}_{file_name}.jpg', input_img)\n",
        "\t\tcv2.imwrite(f'{self.sample_path}/{x_type}_{file_name}_amap.jpg', anomaly_map_norm_hm)\n",
        "\t\t# cv2.imwrite(f'{self.sample_path}/{x_type}_{file_name}_amap_on_img.jpg', hm_on_img)\n",
        "\n",
        "\tdef save_anomaly_map2(self, anomaly_map, input_img, values, names):\n",
        "\t\t\tself.sample_path = f'./sample'\n",
        "\t\t\tos.makedirs(self.sample_path, exist_ok = True)\n",
        "\t\t\tos.makedirs('image log', exist_ok = True)\n",
        "\n",
        "\t\t\tfile_name, folder_name = names\n",
        "\t\t\tmin_value, max_value, (lower, upper) = values\n",
        "\n",
        "\t\t\tlower = np.array(lower, dtype = np.uint8)\n",
        "\t\t\tupper = np.array(upper, dtype = np.uint8)\n",
        "\n",
        "\t\t\ttext_file = open(f'image log/{folder_name}.txt', 'a')\n",
        "\t\t\tif anomaly_map.shape != input_img.shape:\n",
        "\t\t\t\t\tanomaly_map = cv2.resize(anomaly_map, (input_img.shape[0], input_img.shape[1]))\n",
        "\t\t\timage_min, image_max, anomaly_map_norm = min_max_norm2(anomaly_map, min_value, max_value)\n",
        "\n",
        "\t\t\tprint(f'{folder_name}. min : {image_min:.2f}, max : {image_max:.2f}')\n",
        "\t\t\tanomaly_map_norm_hm = cvt2heatmap(anomaly_map_norm*255)\n",
        "\t\t\thm_on_img = heatmap_on_image(anomaly_map_norm_hm, input_img)\n",
        "\n",
        "\t\t\tanomaly_hsv = cv2.cvtColor(anomaly_map_norm_hm, cv2.COLOR_BGR2HSV)\n",
        "\t\t\tmask = cv2.inRange(anomaly_hsv, lower, upper)\n",
        "\t\t\toutput = cv2.bitwise_and(anomaly_map_norm_hm, anomaly_map_norm_hm, mask = mask)\n",
        "\n",
        "\t\t\t_, thr = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\t\t\tdraw_image = input_img.copy()\n",
        "\t\t\tconts, _ = cv2.findContours(thr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "\t\t\tlabel = 'anomaly' if len(conts) != 0 else 'good'\n",
        "\t\t\tfor cont in conts:\n",
        "\t\t\t\t\t# moment = cv2.moments(cont)\n",
        "\t\t\t\t\tcv2.drawContours(draw_image, [cont], 0, (0, 0, 255), 2)\n",
        "\n",
        "\t\t\tsave_path = f'{self.sample_path}/{label}'\n",
        "\t\t\tos.makedirs(save_path, exist_ok = True)\n",
        "\t\t\t# save images\n",
        "\t\t\tcv2.imwrite(os.path.join(save_path, f'{file_name}.jpg'), input_img)\n",
        "\t\t\tcv2.imwrite(os.path.join(save_path, f'{file_name}_mask.jpg'), output)\n",
        "\t\t\tcv2.imwrite(os.path.join(save_path, f'{file_name}_draw.jpg'), draw_image)\n",
        "\t\t\tcv2.imwrite(os.path.join(save_path, f'{file_name}_amap.jpg'), anomaly_map_norm_hm)\n",
        "\t\t\tcv2.imwrite(os.path.join(save_path, f'{file_name}_amap_on_img.jpg'), hm_on_img)\n",
        "\t\t\t\n",
        "\t\t\t# self.label_value_dict[folder_name]['min'].append(image_min)\n",
        "\t\t\t# self.label_value_dict[folder_name]['max'].append(image_max)\n",
        "\n",
        "\t\t\ttext = f'image path : {folder_name}/{file_name}\\nimage pixel minimum : {image_min:.2f}\\nimage pixel maximum : {image_max:.2f}\\n\\n\\n'\n",
        "\t\t\ttext_file.write(text)\n",
        "\t\t\tprint(text)\n",
        "\n",
        "\tdef train_dataloader(self):\n",
        "\t\timage_datasets = MVTecDataset(root=args['dataset_path'], transform=self.data_transforms, phase='train')\n",
        "\t\ttrain_loader = DataLoader(image_datasets, batch_size=args['batch_size'], shuffle=True, num_workers=0) #, pin_memory=True)\n",
        "\t\treturn train_loader\n",
        "\n",
        "\tdef test_dataloader(self):\n",
        "\t\ttest_datasets = MVTecDataset(root=args['dataset_path'], transform=self.data_transforms, phase='test')\n",
        "\t\ttest_loader = DataLoader(test_datasets, batch_size=1, shuffle=False, num_workers=0) #, pin_memory=True) # only work on batch_size=1, now.\n",
        "\t\treturn test_loader\n",
        "\n",
        "\tdef configure_optimizers(self):\n",
        "\t\treturn None\n",
        "\n",
        "\tdef on_train_start(self):\n",
        "\t\tself.model.eval() # to stop running_var move (maybe not critical)\n",
        "\t\tself.embedding_dir_path, self.sample_path, self.source_code_save_path = prep_dirs(self.logger.log_dir)\n",
        "\t\tself.embedding_list = []\n",
        "\t\n",
        "\tdef on_test_start(self):\n",
        "\t\tself.init_results_list()\n",
        "\t\tself.embedding_dir_path, self.sample_path, self.source_code_save_path = prep_dirs(self.logger.log_dir)\n",
        "\t\t\n",
        "\tdef training_step(self, batch, batch_idx): # save locally aware patch features\n",
        "\t\tx, _, file_name, _ = batch\n",
        "\t\tfeatures = self(x)\n",
        "\t\tembeddings = []\n",
        "\t\tfor feature in features:\n",
        "\t\t\tm = torch.nn.AvgPool2d(3, 1, 1)\n",
        "\t\t\tembeddings.append(m(feature))\n",
        "\t\tembedding = embedding_concat(embeddings[0], embeddings[1])\n",
        "\t\tself.embedding_list.extend(reshape_embedding(np.array(embedding)))\n",
        "\n",
        "\tdef training_epoch_end(self, outputs): \n",
        "\t\ttotal_embeddings = np.array(self.embedding_list)\n",
        "\t\t# Random projection\n",
        "\t\tself.randomprojector = SparseRandomProjection(n_components='auto', eps=0.9) # 'auto' => Johnson-Lindenstrauss lemma\n",
        "\t\tself.randomprojector.fit(total_embeddings)\n",
        "\t\t# Coreset Subsampling\n",
        "\t\tselector = kCenterGreedy(total_embeddings,0,0)\n",
        "\t\tselected_idx = selector.select_batch(model=self.randomprojector, already_selected=[], N=int(total_embeddings.shape[0]*args['coreset_sampling_ratio']))\n",
        "\t\tself.embedding_coreset = total_embeddings[selected_idx]\n",
        "\t\t\n",
        "\t\tprint('initial embedding size : ', total_embeddings.shape)\n",
        "\t\tprint('final embedding size : ', self.embedding_coreset.shape)\n",
        "\t\twith open(os.path.join(self.embedding_dir_path, 'embedding.pickle'), 'wb') as f:\n",
        "\t\t\tpickle.dump(self.embedding_coreset, f)\n",
        "\n",
        "\t\ttorch.save(model.state_dict(), f'{args[\"model_path\"]}/model.pt')\n",
        "\n",
        "\tdef test_step(self, batch, batch_idx): # Nearest Neighbour Search\n",
        "\t\tself.embedding_coreset = pickle.load(open(os.path.join(self.embedding_dir_path, 'embedding.pickle'), 'rb'))\n",
        "\t\tx, label, file_name, x_type = batch\n",
        "\t\t# extract embedding\n",
        "\t\tfeatures = self(x)\n",
        "\t\tembeddings = []\n",
        "\t\tfor feature in features:\n",
        "\t\t\tm = torch.nn.AvgPool2d(3, 1, 1)\n",
        "\t\t\tembeddings.append(m(feature))\n",
        "\t\tembedding_ = embedding_concat(embeddings[0], embeddings[1])\n",
        "\t\tembedding_test = np.array(reshape_embedding(np.array(embedding_)))\n",
        "\t\t# NN\n",
        "\t\tknn = KNN(torch.from_numpy(self.embedding_coreset).cuda(), k=9)\n",
        "\t\tscore_patches = knn(torch.from_numpy(embedding_test).cuda())[0].cpu().detach().numpy()\n",
        "\n",
        "\t\tanomaly_map = score_patches[:,0].reshape((28,28))\n",
        "\t\tN_b = score_patches[np.argmax(score_patches[:,0])]\n",
        "\t\tw = (1 - (np.max(np.exp(N_b))/np.sum(np.exp(N_b))))\n",
        "\t\tscore = w*max(score_patches[:,0]) # Image-level score\n",
        "\t\t\n",
        "\t\tanomaly_map_resized = cv2.resize(anomaly_map, (args['input_size'], args['input_size']))\n",
        "\t\tanomaly_map_resized_blur = gaussian_filter(anomaly_map_resized, sigma=4)\n",
        "\t\t\n",
        "\t\tself.pred_list_px_lvl.extend(anomaly_map_resized_blur.ravel())\n",
        "\t\tself.pred_list_img_lvl.append(score)\n",
        "\t\tself.img_path_list.extend(file_name)\n",
        "\t\t# save images\n",
        "\t\t# x = self.inv_normalize(x)\n",
        "\t\tinput_x = cv2.cvtColor(x.permute(0,2,3,1).cpu().numpy()[0]*255, cv2.COLOR_BGR2RGB)\n",
        "\t\tself.save_anomaly_map(anomaly_map_resized_blur, input_x, file_name[0], x_type[0])\n",
        "  \n",
        "\t\tprint(score)\n",
        "\tdef inspection_image(self, embedding, batch, values): # Nearest Neighbour Search\n",
        "\n",
        "\t\t\tx, file_name, folder_name = batch\n",
        "\n",
        "\t\t\t# extract embedding\n",
        "\t\t\tx = x.unsqueeze(0)\n",
        "\t\t\tfeatures = self(x)\n",
        "\t\t\tembeddings = []\n",
        "\t\t\tfor feature in features:\n",
        "\t\t\t\t\tm = torch.nn.AvgPool2d(3, 1, 1)\n",
        "\t\t\t\t\tembeddings.append(m(feature))\n",
        "\t\t\tembedding_ = embedding_concat(embeddings[0], embeddings[1])\n",
        "\t\t\tembedding_test = np.array(reshape_embedding(np.array(embedding_)))\n",
        "\n",
        "\t\t\t# NN\n",
        "\t\t\t# knn = KNN(torch.from_numpy(self.embedding_coreset).cuda(), k=9)\n",
        "\t\t\t# score_patches = knn(torch.from_numpy(embedding_test).cuda())[0].cpu().detach().numpy()\n",
        "\t\t\tknn = KNN(torch.from_numpy(embedding), k=9)\n",
        "\t\t\tscore_patches = knn(torch.from_numpy(embedding_test))[0].cpu().detach().numpy()\n",
        "\n",
        "\t\t\tanomaly_map = score_patches[:,0].reshape((28,28))\n",
        "\t\t\tanomaly_map_resized = cv2.resize(anomaly_map, (args.input_size, args.input_size))\n",
        "\t\t\tanomaly_map_resized_blur = gaussian_filter(anomaly_map_resized, sigma=4)\n",
        "\n",
        "\t\t\t# x = self.inv_normalize(x)\n",
        "\t\t\tinput_x = cv2.cvtColor(x.permute(0,2,3,1).cpu().numpy()[0]*255, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tself.save_anomaly_map2(anomaly_map_resized_blur, input_x, values ,(file_name, folder_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4liGi6lVDBb"
      },
      "source": [
        "config = {}\n",
        "def get_args():\n",
        "  config['phase'] = 'train'\n",
        "  config['dataset_path'] = 'dataset'\n",
        "  config['epochs'] = 3\n",
        "  config['batch_size'] = 2\n",
        "  config['load_size'] = 256\n",
        "  config['input_size'] = 224\n",
        "  config['coreset_sampling_ratio'] = 0.001\n",
        "  config['output_path'] = 'output'\n",
        "  config['save_anomaly_map'] = True\n",
        "  config['n_neighbors'] = 9\n",
        "  config['model_path'] = 'model'\n",
        "  return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dX1p6x24Unj"
      },
      "source": [
        "!sudo rm -rf model output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b312bea609d940b8bac330eeb5e87289",
            "aa809a24e29f4d5c878624aa51b9ce86",
            "4bb381671ebc44129f78fc3d08aa3265",
            "cfb3da9e83294645baf141807fc1eb01",
            "d24c458722e3429bb238d0f8bbe35bcb",
            "d0229e1b3fa9463a9acc0c7de149298c",
            "31b6ff1b1a7b47dfbc65d4d4c2eea0f1",
            "945960b992854559b9ad2916c64814d5",
            "f81c60e4aa344879bf20ac7c130267d3",
            "94fe01d368ef4f47a31d2207113cad5a",
            "a0d480a52e27417c9138654e1d71d9dc",
            "846192b2a0c24cd9864886ba306c7811",
            "e2fec08c99f349d292c8fb3738106950",
            "0dbb64ae59df4fa4be7c4d41636c56f1",
            "cb6546774f5044e6b061879bae86604a",
            "a2310b4fe73b4f188f1ab7a887dec7a6",
            "4c6f8fdc0303467c83d9ba9e13619727",
            "863d58abbb4045bab5b586e9d9e668a5",
            "c8bba117d2354d85882a09602c94feda",
            "d9ccb795959d45ca87cc3b51aae5eb40",
            "e3daca2246794a58b3b579d188f19081",
            "ca09c023f42346b5ae6033d8ad3b5028"
          ]
        },
        "id": "oDWFzNUmVKkJ",
        "outputId": "e899c794-4bd2-4642-bf50-e8bc79ad485b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from easydict import EasyDict\n",
        "args = EasyDict(get_args())\n",
        "\t\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
        "\n",
        "print('학습을 진행하는 기기:', device)\n",
        "print('gpu 사용 가능? : ', USE_CUDA, '사용 중인 gpu 이름 : ', gpu_name)\n",
        "\n",
        "trainer = pl.Trainer.from_argparse_args(args, default_root_dir=args['output_path'], max_epochs=args['epochs'], gpus=1)\n",
        "model = STPM(hparams=args)\n",
        "\n",
        "os.makedirs(args['model_path'], exist_ok = True)\n",
        "if args['phase'] == 'train':\n",
        "  trainer.fit(model)\t\t\n",
        "  trainer.test(model)\n",
        "elif args['pahse'] == 'test':\n",
        "  trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습을 진행하는 기기: cuda:0\n",
            "gpu 사용 가능? :  True 사용 중인 gpu 이름 :  Tesla P100-PCIE-16GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/optimizers.py:38: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
            "  UserWarning,\n",
            "\n",
            "  | Name      | Type    | Params\n",
            "--------------------------------------\n",
            "0 | model     | ResNet  | 68.9 M\n",
            "1 | criterion | MSELoss | 0     \n",
            "--------------------------------------\n",
            "0         Trainable params\n",
            "68.9 M    Non-trainable params\n",
            "68.9 M    Total params\n",
            "275.533   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b312bea609d940b8bac330eeb5e87289",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: -1it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py:559: UserWarning: training_step returned None. If this was on purpose, ignore this warning...\n",
            "  \"training_step returned None. If this was on purpose, ignore this warning...\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting transformed features...\n",
            "Calculating distances...\n",
            "Maximum distance from cluster centers is 2.97\n",
            "initial embedding size :  (196000, 1536)\n",
            "final embedding size :  (196, 1536)\n",
            "Getting transformed features...\n",
            "Calculating distances...\n",
            "Maximum distance from cluster centers is 2.79\n",
            "initial embedding size :  (392000, 1536)\n",
            "final embedding size :  (392, 1536)\n",
            "Getting transformed features...\n",
            "Calculating distances...\n",
            "Maximum distance from cluster centers is 2.70\n",
            "initial embedding size :  (588000, 1536)\n",
            "final embedding size :  (588, 1536)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "846192b2a0c24cd9864886ba306c7811",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-161f181f7ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pahse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, test_dataloaders)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_evaluating\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_evaluating\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# double dispatch to initiate the test loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"run_{self.state.stage}_evaluation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0meval_loop_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0;31m# remove the tensors from the eval results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         dl_outputs = self.epoch_loop.run(\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_dataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-927e3173bcdc>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;31m# x = self.inv_normalize(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_anomaly_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_map_resized_blur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-927e3173bcdc>\u001b[0m in \u001b[0;36msave_anomaly_map\u001b[0;34m(self, anomaly_map, input_img, file_name, x_type)\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0manomaly_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0manomaly_map_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0manomaly_map_norm_hm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvt2heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_map_norm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# print(anomaly_map_norm_hm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-39b6b8073df8>\u001b[0m in \u001b[0;36mcvt2heatmap\u001b[0;34m(gray)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcvt2heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLORMAP_JET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bG9ox8RhJ9ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB-jnwlbxp3p",
        "outputId": "7f12a78d-c240-48d4-be33-9a68ada451ea"
      },
      "source": [
        "!ls model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding.pickle  model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEo_oq2GsHyH"
      },
      "source": [
        "def load_model(model_path, embedding_path):\n",
        "    model = STPM(hparams = args)\n",
        "    model.load_state_dict(torch.load(model_path), strict = False)\n",
        "    model.eval()\n",
        "    \n",
        "    embedding_coreset = pickle.load(open(embedding_path, 'rb'))\n",
        "    return model, embedding_coreset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_J9fohoiPXy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9671cfa5-025f-4cbe-d04a-7d6de5f13b94"
      },
      "source": [
        "from imutils import paths\n",
        "from easydict import EasyDict\n",
        "args = EasyDict(get_args())\n",
        "\n",
        "print(f'{args[\"model_path\"]}/{args[\"model_path\"]}.pt')\n",
        "model, embedding = load_model('model/model.pt', 'model/embedding.pickle')\n",
        "print(model)\n",
        "\n",
        "image_paths = sorted(paths.list_images('dataset/test/good'))\n",
        "min_value, max_value = 0.32, 3.4 #bow tape data\n",
        "boundary = [[0, 25, 25], [20, 255, 255]]\n",
        "\n",
        "for idx, image_path in enumerate(image_paths, 1):\n",
        "  file_name = image_path.split(os.path.sep)[-1]\n",
        "  folder_name = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  image = Image.fromarray(image)\n",
        "  data_transform =transforms.Compose([\n",
        "              transforms.Resize((args.load_size, args.load_size), Image.ANTIALIAS),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.CenterCrop(args.input_size),\n",
        "              transforms.Normalize(mean=mean_train,\n",
        "                                  std=std_train)])\n",
        "\n",
        "  image = data_transform(image)\n",
        "  batch = [image, file_name, folder_name]\n",
        "  model.inspection_image(embedding, batch, (min_value, max_value, boundary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model/model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STPM(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            "  )\n",
            "  (criterion): MSELoss()\n",
            ")\n",
            "good. min : 1.27, max : 3.32\n",
            "image path : good/no1000.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 2.18, max : 3.43\n",
            "image path : good/no1001.jpg\n",
            "image pixel minimum : 2.18\n",
            "image pixel maximum : 3.43\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.38, max : 3.15\n",
            "image path : good/no1002.jpg\n",
            "image pixel minimum : 1.38\n",
            "image pixel maximum : 3.15\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.72\n",
            "image path : good/no1003.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.72\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.26, max : 3.49\n",
            "image path : good/no1004.jpg\n",
            "image pixel minimum : 1.26\n",
            "image pixel maximum : 3.49\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.06, max : 3.22\n",
            "image path : good/no1005.jpg\n",
            "image pixel minimum : 1.06\n",
            "image pixel maximum : 3.22\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.56\n",
            "image path : good/no1006.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.56\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.31\n",
            "image path : good/no1007.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.31\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.38, max : 3.32\n",
            "image path : good/no1008.jpg\n",
            "image pixel minimum : 1.38\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.46, max : 3.35\n",
            "image path : good/no1009.jpg\n",
            "image pixel minimum : 1.46\n",
            "image pixel maximum : 3.35\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.70, max : 3.28\n",
            "image path : good/no1010.jpg\n",
            "image pixel minimum : 1.70\n",
            "image pixel maximum : 3.28\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.62, max : 3.19\n",
            "image path : good/no1011.jpg\n",
            "image pixel minimum : 1.62\n",
            "image pixel maximum : 3.19\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.80, max : 3.33\n",
            "image path : good/no1012.jpg\n",
            "image pixel minimum : 1.80\n",
            "image pixel maximum : 3.33\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 4.19\n",
            "image path : good/no1013.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 4.19\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.22, max : 3.79\n",
            "image path : good/no1014.jpg\n",
            "image pixel minimum : 1.22\n",
            "image pixel maximum : 3.79\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.29, max : 3.44\n",
            "image path : good/no1015.jpg\n",
            "image pixel minimum : 1.29\n",
            "image pixel maximum : 3.44\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.26\n",
            "image path : good/no1016.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.26\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.85\n",
            "image path : good/no1017.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.85\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 3.49\n",
            "image path : good/no1018.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 3.49\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.13\n",
            "image path : good/no1019.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.13\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.47, max : 3.48\n",
            "image path : good/no1020.jpg\n",
            "image pixel minimum : 1.47\n",
            "image pixel maximum : 3.48\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.46, max : 3.25\n",
            "image path : good/no1021.jpg\n",
            "image pixel minimum : 1.46\n",
            "image pixel maximum : 3.25\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.40, max : 3.23\n",
            "image path : good/no1022.jpg\n",
            "image pixel minimum : 1.40\n",
            "image pixel maximum : 3.23\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 3.39\n",
            "image path : good/no1023.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 3.39\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.76, max : 3.49\n",
            "image path : good/no1024.jpg\n",
            "image pixel minimum : 1.76\n",
            "image pixel maximum : 3.49\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.43, max : 3.43\n",
            "image path : good/no1025.jpg\n",
            "image pixel minimum : 1.43\n",
            "image pixel maximum : 3.43\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 3.48\n",
            "image path : good/no1026.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 3.48\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.81, max : 3.31\n",
            "image path : good/no1027.jpg\n",
            "image pixel minimum : 1.81\n",
            "image pixel maximum : 3.31\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.19, max : 3.34\n",
            "image path : good/no1028.jpg\n",
            "image pixel minimum : 1.19\n",
            "image pixel maximum : 3.34\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.32\n",
            "image path : good/no1029.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.04, max : 3.44\n",
            "image path : good/no1030.jpg\n",
            "image pixel minimum : 1.04\n",
            "image pixel maximum : 3.44\n",
            "\n",
            "\n",
            "\n",
            "good. min : 2.14, max : 3.46\n",
            "image path : good/no1031.jpg\n",
            "image pixel minimum : 2.14\n",
            "image pixel maximum : 3.46\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.46\n",
            "image path : good/no1032.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.46\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.50, max : 3.09\n",
            "image path : good/no1033.jpg\n",
            "image pixel minimum : 1.50\n",
            "image pixel maximum : 3.09\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.68, max : 3.10\n",
            "image path : good/no1034.jpg\n",
            "image pixel minimum : 1.68\n",
            "image pixel maximum : 3.10\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.74, max : 3.36\n",
            "image path : good/no1035.jpg\n",
            "image pixel minimum : 1.74\n",
            "image pixel maximum : 3.36\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.19, max : 3.32\n",
            "image path : good/no1036.jpg\n",
            "image pixel minimum : 1.19\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.28\n",
            "image path : good/no1037.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.28\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 3.67\n",
            "image path : good/no1038.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.99, max : 3.49\n",
            "image path : good/no1039.jpg\n",
            "image pixel minimum : 0.99\n",
            "image pixel maximum : 3.49\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.20, max : 3.51\n",
            "image path : good/no1040.jpg\n",
            "image pixel minimum : 1.20\n",
            "image pixel maximum : 3.51\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.52\n",
            "image path : good/no1041.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.52\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.30\n",
            "image path : good/no1042.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.38\n",
            "image path : good/no1043.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.38\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.06, max : 3.14\n",
            "image path : good/no1044.jpg\n",
            "image pixel minimum : 1.06\n",
            "image pixel maximum : 3.14\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.79, max : 3.36\n",
            "image path : good/no1045.jpg\n",
            "image pixel minimum : 1.79\n",
            "image pixel maximum : 3.36\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.22, max : 3.67\n",
            "image path : good/no1046.jpg\n",
            "image pixel minimum : 1.22\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.29\n",
            "image path : good/no1047.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.29\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.22\n",
            "image path : good/no1048.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.22\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.43, max : 3.40\n",
            "image path : good/no1049.jpg\n",
            "image pixel minimum : 1.43\n",
            "image pixel maximum : 3.40\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.41, max : 3.20\n",
            "image path : good/no1050.jpg\n",
            "image pixel minimum : 1.41\n",
            "image pixel maximum : 3.20\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.32, max : 3.27\n",
            "image path : good/no1051.jpg\n",
            "image pixel minimum : 1.32\n",
            "image pixel maximum : 3.27\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.39, max : 3.10\n",
            "image path : good/no1052.jpg\n",
            "image pixel minimum : 1.39\n",
            "image pixel maximum : 3.10\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.39\n",
            "image path : good/no1053.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.39\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.31, max : 3.24\n",
            "image path : good/no1054.jpg\n",
            "image pixel minimum : 1.31\n",
            "image pixel maximum : 3.24\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.92, max : 3.47\n",
            "image path : good/no1055.jpg\n",
            "image pixel minimum : 1.92\n",
            "image pixel maximum : 3.47\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.26, max : 4.04\n",
            "image path : good/no1056.jpg\n",
            "image pixel minimum : 1.26\n",
            "image pixel maximum : 4.04\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.19, max : 3.30\n",
            "image path : good/no1057.jpg\n",
            "image pixel minimum : 1.19\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.33, max : 3.23\n",
            "image path : good/no1058.jpg\n",
            "image pixel minimum : 1.33\n",
            "image pixel maximum : 3.23\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.32, max : 3.27\n",
            "image path : good/no1059.jpg\n",
            "image pixel minimum : 1.32\n",
            "image pixel maximum : 3.27\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.32\n",
            "image path : good/no1060.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.48, max : 3.30\n",
            "image path : good/no1061.jpg\n",
            "image pixel minimum : 1.48\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.64, max : 3.52\n",
            "image path : good/no1062.jpg\n",
            "image pixel minimum : 1.64\n",
            "image pixel maximum : 3.52\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.98, max : 3.59\n",
            "image path : good/no1063.jpg\n",
            "image pixel minimum : 0.98\n",
            "image pixel maximum : 3.59\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.53\n",
            "image path : good/no1064.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.53\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.69, max : 3.20\n",
            "image path : good/no1065.jpg\n",
            "image pixel minimum : 1.69\n",
            "image pixel maximum : 3.20\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.26, max : 3.30\n",
            "image path : good/no1066.jpg\n",
            "image pixel minimum : 1.26\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.41, max : 3.04\n",
            "image path : good/no1067.jpg\n",
            "image pixel minimum : 1.41\n",
            "image pixel maximum : 3.04\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.24, max : 3.37\n",
            "image path : good/no1068.jpg\n",
            "image pixel minimum : 1.24\n",
            "image pixel maximum : 3.37\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.33, max : 3.07\n",
            "image path : good/no1069.jpg\n",
            "image pixel minimum : 1.33\n",
            "image pixel maximum : 3.07\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.90, max : 3.33\n",
            "image path : good/no1070.jpg\n",
            "image pixel minimum : 0.90\n",
            "image pixel maximum : 3.33\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.39, max : 3.59\n",
            "image path : good/no1071.jpg\n",
            "image pixel minimum : 1.39\n",
            "image pixel maximum : 3.59\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.22, max : 3.32\n",
            "image path : good/no1072.jpg\n",
            "image pixel minimum : 1.22\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.05, max : 3.35\n",
            "image path : good/no1073.jpg\n",
            "image pixel minimum : 1.05\n",
            "image pixel maximum : 3.35\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.72, max : 3.77\n",
            "image path : good/no1074.jpg\n",
            "image pixel minimum : 1.72\n",
            "image pixel maximum : 3.77\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.02, max : 3.29\n",
            "image path : good/no1075.jpg\n",
            "image pixel minimum : 1.02\n",
            "image pixel maximum : 3.29\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.40, max : 3.14\n",
            "image path : good/no1076.jpg\n",
            "image pixel minimum : 1.40\n",
            "image pixel maximum : 3.14\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.39, max : 3.02\n",
            "image path : good/no1077.jpg\n",
            "image pixel minimum : 1.39\n",
            "image pixel maximum : 3.02\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.65, max : 3.30\n",
            "image path : good/no1078.jpg\n",
            "image pixel minimum : 1.65\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.22, max : 3.66\n",
            "image path : good/no1079.jpg\n",
            "image pixel minimum : 1.22\n",
            "image pixel maximum : 3.66\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.26\n",
            "image path : good/no1080.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.26\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.38, max : 3.55\n",
            "image path : good/no1081.jpg\n",
            "image pixel minimum : 1.38\n",
            "image pixel maximum : 3.55\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.63, max : 3.71\n",
            "image path : good/no1082.jpg\n",
            "image pixel minimum : 1.63\n",
            "image pixel maximum : 3.71\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.40, max : 3.82\n",
            "image path : good/no1083.jpg\n",
            "image pixel minimum : 1.40\n",
            "image pixel maximum : 3.82\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.68\n",
            "image path : good/no1084.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.68\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.55, max : 3.68\n",
            "image path : good/no1085.jpg\n",
            "image pixel minimum : 1.55\n",
            "image pixel maximum : 3.68\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.57, max : 3.41\n",
            "image path : good/no1086.jpg\n",
            "image pixel minimum : 1.57\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.71\n",
            "image path : good/no1087.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.71\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.76, max : 3.55\n",
            "image path : good/no1088.jpg\n",
            "image pixel minimum : 1.76\n",
            "image pixel maximum : 3.55\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.68, max : 3.32\n",
            "image path : good/no1089.jpg\n",
            "image pixel minimum : 1.68\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.98, max : 3.62\n",
            "image path : good/no1090.jpg\n",
            "image pixel minimum : 0.98\n",
            "image pixel maximum : 3.62\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.63\n",
            "image path : good/no1091.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.63\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.41\n",
            "image path : good/no1092.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.86, max : 3.58\n",
            "image path : good/no1093.jpg\n",
            "image pixel minimum : 1.86\n",
            "image pixel maximum : 3.58\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.41, max : 3.35\n",
            "image path : good/no1094.jpg\n",
            "image pixel minimum : 1.41\n",
            "image pixel maximum : 3.35\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.67\n",
            "image path : good/no1095.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.55\n",
            "image path : good/no1096.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.55\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.55, max : 3.41\n",
            "image path : good/no1097.jpg\n",
            "image pixel minimum : 1.55\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.54\n",
            "image path : good/no1098.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.54\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.73, max : 3.59\n",
            "image path : good/no1099.jpg\n",
            "image pixel minimum : 1.73\n",
            "image pixel maximum : 3.59\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.42, max : 3.30\n",
            "image path : good/no1100.jpg\n",
            "image pixel minimum : 1.42\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.47, max : 3.56\n",
            "image path : good/no1101.jpg\n",
            "image pixel minimum : 1.47\n",
            "image pixel maximum : 3.56\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.16, max : 3.65\n",
            "image path : good/no1102.jpg\n",
            "image pixel minimum : 1.16\n",
            "image pixel maximum : 3.65\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.67\n",
            "image path : good/no1103.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.59, max : 3.53\n",
            "image path : good/no1104.jpg\n",
            "image pixel minimum : 1.59\n",
            "image pixel maximum : 3.53\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.35, max : 3.68\n",
            "image path : good/no1105.jpg\n",
            "image pixel minimum : 1.35\n",
            "image pixel maximum : 3.68\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.34, max : 3.75\n",
            "image path : good/no1106.jpg\n",
            "image pixel minimum : 1.34\n",
            "image pixel maximum : 3.75\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.34, max : 3.61\n",
            "image path : good/no1107.jpg\n",
            "image pixel minimum : 1.34\n",
            "image pixel maximum : 3.61\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.91, max : 3.61\n",
            "image path : good/no1108.jpg\n",
            "image pixel minimum : 0.91\n",
            "image pixel maximum : 3.61\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.71\n",
            "image path : good/no1109.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.71\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.65, max : 3.48\n",
            "image path : good/no1110.jpg\n",
            "image pixel minimum : 1.65\n",
            "image pixel maximum : 3.48\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.85, max : 3.43\n",
            "image path : good/no1111.jpg\n",
            "image pixel minimum : 0.85\n",
            "image pixel maximum : 3.43\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.30, max : 3.25\n",
            "image path : good/no1112.jpg\n",
            "image pixel minimum : 1.30\n",
            "image pixel maximum : 3.25\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.56, max : 3.56\n",
            "image path : good/no1113.jpg\n",
            "image pixel minimum : 1.56\n",
            "image pixel maximum : 3.56\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.45, max : 3.32\n",
            "image path : good/no1114.jpg\n",
            "image pixel minimum : 1.45\n",
            "image pixel maximum : 3.32\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.55, max : 3.53\n",
            "image path : good/no1115.jpg\n",
            "image pixel minimum : 1.55\n",
            "image pixel maximum : 3.53\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.36\n",
            "image path : good/no1116.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.36\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.69, max : 3.47\n",
            "image path : good/no1117.jpg\n",
            "image pixel minimum : 1.69\n",
            "image pixel maximum : 3.47\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.67, max : 3.42\n",
            "image path : good/no1118.jpg\n",
            "image pixel minimum : 1.67\n",
            "image pixel maximum : 3.42\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.26, max : 3.84\n",
            "image path : good/no1119.jpg\n",
            "image pixel minimum : 1.26\n",
            "image pixel maximum : 3.84\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.96, max : 3.87\n",
            "image path : good/no1120.jpg\n",
            "image pixel minimum : 1.96\n",
            "image pixel maximum : 3.87\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.22, max : 3.63\n",
            "image path : good/no1121.jpg\n",
            "image pixel minimum : 1.22\n",
            "image pixel maximum : 3.63\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.21, max : 3.72\n",
            "image path : good/no1122.jpg\n",
            "image pixel minimum : 1.21\n",
            "image pixel maximum : 3.72\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.47\n",
            "image path : good/no1123.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.47\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.21, max : 3.76\n",
            "image path : good/no1124.jpg\n",
            "image pixel minimum : 1.21\n",
            "image pixel maximum : 3.76\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.36, max : 3.66\n",
            "image path : good/no1125.jpg\n",
            "image pixel minimum : 1.36\n",
            "image pixel maximum : 3.66\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.53, max : 3.41\n",
            "image path : good/no1126.jpg\n",
            "image pixel minimum : 1.53\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.70, max : 3.60\n",
            "image path : good/no1127.jpg\n",
            "image pixel minimum : 1.70\n",
            "image pixel maximum : 3.60\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.47\n",
            "image path : good/no1128.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.47\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.62\n",
            "image path : good/no1129.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.62\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.32, max : 3.37\n",
            "image path : good/no1130.jpg\n",
            "image pixel minimum : 1.32\n",
            "image pixel maximum : 3.37\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.11, max : 3.67\n",
            "image path : good/no1131.jpg\n",
            "image pixel minimum : 1.11\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.90, max : 3.62\n",
            "image path : good/no1132.jpg\n",
            "image pixel minimum : 1.90\n",
            "image pixel maximum : 3.62\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.41\n",
            "image path : good/no1133.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.09, max : 3.16\n",
            "image path : good/no1134.jpg\n",
            "image pixel minimum : 1.09\n",
            "image pixel maximum : 3.16\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.20, max : 3.71\n",
            "image path : good/no1135.jpg\n",
            "image pixel minimum : 1.20\n",
            "image pixel maximum : 3.71\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.27, max : 3.63\n",
            "image path : good/no1136.jpg\n",
            "image pixel minimum : 1.27\n",
            "image pixel maximum : 3.63\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.45, max : 3.45\n",
            "image path : good/no1137.jpg\n",
            "image pixel minimum : 1.45\n",
            "image pixel maximum : 3.45\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.71, max : 3.60\n",
            "image path : good/no1138.jpg\n",
            "image pixel minimum : 1.71\n",
            "image pixel maximum : 3.60\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.48, max : 3.43\n",
            "image path : good/no1139.jpg\n",
            "image pixel minimum : 1.48\n",
            "image pixel maximum : 3.43\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.78, max : 3.30\n",
            "image path : good/no1140.jpg\n",
            "image pixel minimum : 1.78\n",
            "image pixel maximum : 3.30\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.45, max : 3.75\n",
            "image path : good/no1141.jpg\n",
            "image pixel minimum : 1.45\n",
            "image pixel maximum : 3.75\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.67\n",
            "image path : good/no1142.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.67\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.32, max : 3.74\n",
            "image path : good/no1143.jpg\n",
            "image pixel minimum : 1.32\n",
            "image pixel maximum : 3.74\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.35, max : 3.64\n",
            "image path : good/no1144.jpg\n",
            "image pixel minimum : 1.35\n",
            "image pixel maximum : 3.64\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.61\n",
            "image path : good/no1145.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.61\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.23, max : 3.55\n",
            "image path : good/no1146.jpg\n",
            "image pixel minimum : 1.23\n",
            "image pixel maximum : 3.55\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.45, max : 3.52\n",
            "image path : good/no1147.jpg\n",
            "image pixel minimum : 1.45\n",
            "image pixel maximum : 3.52\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.92, max : 3.62\n",
            "image path : good/no1148.jpg\n",
            "image pixel minimum : 0.92\n",
            "image pixel maximum : 3.62\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.28, max : 3.64\n",
            "image path : good/no1149.jpg\n",
            "image pixel minimum : 1.28\n",
            "image pixel maximum : 3.64\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.75, max : 3.41\n",
            "image path : good/no1150.jpg\n",
            "image pixel minimum : 1.75\n",
            "image pixel maximum : 3.41\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.25, max : 3.52\n",
            "image path : good/no1151.jpg\n",
            "image pixel minimum : 1.25\n",
            "image pixel maximum : 3.52\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.38, max : 3.24\n",
            "image path : good/no1152.jpg\n",
            "image pixel minimum : 1.38\n",
            "image pixel maximum : 3.24\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.43, max : 3.43\n",
            "image path : good/no1153.jpg\n",
            "image pixel minimum : 1.43\n",
            "image pixel maximum : 3.43\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.98, max : 3.44\n",
            "image path : good/no1154.jpg\n",
            "image pixel minimum : 0.98\n",
            "image pixel maximum : 3.44\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.52, max : 3.56\n",
            "image path : good/no1155.jpg\n",
            "image pixel minimum : 1.52\n",
            "image pixel maximum : 3.56\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.29, max : 3.35\n",
            "image path : good/no1156.jpg\n",
            "image pixel minimum : 1.29\n",
            "image pixel maximum : 3.35\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.17, max : 3.29\n",
            "image path : good/no1157.jpg\n",
            "image pixel minimum : 1.17\n",
            "image pixel maximum : 3.29\n",
            "\n",
            "\n",
            "\n",
            "good. min : 0.97, max : 3.40\n",
            "image path : good/no1158.jpg\n",
            "image pixel minimum : 0.97\n",
            "image pixel maximum : 3.40\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.26, max : 3.40\n",
            "image path : good/no1159.jpg\n",
            "image pixel minimum : 1.26\n",
            "image pixel maximum : 3.40\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.59, max : 3.39\n",
            "image path : good/no1160.jpg\n",
            "image pixel minimum : 1.59\n",
            "image pixel maximum : 3.39\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.54, max : 3.36\n",
            "image path : good/no1161.jpg\n",
            "image pixel minimum : 1.54\n",
            "image pixel maximum : 3.36\n",
            "\n",
            "\n",
            "\n",
            "good. min : 1.66, max : 3.52\n",
            "image path : good/no1162.jpg\n",
            "image pixel minimum : 1.66\n",
            "image pixel maximum : 3.52\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fc64c9f58f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspection_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-927e3173bcdc>\u001b[0m in \u001b[0;36minspection_image\u001b[0;34m(self, embedding, batch, values)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         \u001b[0;31m# extract embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-927e3173bcdc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_t)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVAKonwPaMtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}